# -*- coding: utf-8 -*-
"""SoC_Week4_Assg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/167NsOV3i5lXTEoKD4FPXn11oYIWq7E1k
"""

import numpy as np
import random
import matplotlib.pyplot as plt
import pandas as pd

points = [(1,4), (2,5), (1,2), (4,9), (0,2), (9,3), (3,4), (2,7), (3,9), (1,9), (5,8), (3,2), (2.3,4.9), (3.1,6.4),
(4.1,2.3), (2.6,8.5), (3.9, 2.3), (8,1), (2,5), (3,3), (4,4.5), (7.6,6.7), (8.4,4.8), (9.1,1.9)]

df = pd.DataFrame(points)  # Converting the series into a dataframe
df.shape

# Import required module
from sklearn.cluster import KMeans
 
# Initialize the class object
kmeans = KMeans(n_clusters= 3)
 
# Predict the labels of clusters.
label = kmeans.fit_predict(df)
 
print(label)

def split(arr, cond):
  return [arr[cond], arr[~cond]]

import matplotlib.pyplot as plt
 
#filter rows of original data
filtered_label0 = df[label == 0]
 
#plotting the results
plt.scatter(filtered_label0[:,0] , filtered_label0[:,1])
plt.show()

#Getting unique labels
 
u_labels = np.unique(label)
 
#plotting the results:
 
for i in u_labels:
    x = []
    y = []
    for j in range(len(points)):
        if(label[j] == i):
            x.append(points[0])
            y.append(points[1])
        plt.scatter(x,y)
plt.legend()
plt.show()

# Getting unique labels
 
u_labels = np.unique(label)
 
# Plotting the results:
 
for i in u_labels:
    cluster = split(points, df[label] == i )
    x = cluster[0]
    y = cluster[1]
    plt.scatter(x,y)
plt.legend()
plt.show()

:# This was an attempt to write the code myself. However, later I have used functions from the scikit library, so you may ignore this cell. 

def assign(array1, array2):
    dist = [] # Array of distances of each point from each centroid, so this array has k elements
    c_i = []  # Array of clusters each point is assigned 
    
    for i in array1:  # array1 contains the points
        for j in len(array2):  # array2 contains the cluster centroids
            cluster[j] = []    # Points belonging to the jth cluster
            dist.append(numpy.linalg.norm(i-array2[j]))
        c_i = dist.index(min(dist))  
        cluster[c_i].append(i)  # Assigning the point to the cluster with the closest centroid
    return c_i, cluster

from sklearn.cluster import KMeans
import numpy as np
X = np.array([(1,4), (2,5), (1,2), (4,9), (0,2), (9,3), (3,4), (2,7), (3,9), (1,9), (5,8), (3,2), (2.3,4.9), (3.1,6.4),
(4.1,2.3), (2.6,8.5), (3.9, 2.3), (8,1), (2,5), (3,3), (4,4.5), (7.6,6.7), (8.4,4.8), (9.1,1.9)])
kmeans = KMeans(n_clusters=5, random_state=0, n_init="auto").fit(X)
final = np.array(kmeans.labels_)
final

k = len(np.unique(final))
k

for i in range(k):
    cluster[i] = []
    x[i] = cluster[i][0]
    y[i] = cluster[i][1]
    for j in range(final):
        if(j == i):
            cluster[i].append(j)
    plt.scatter(x, y)

x = []
y = []

for i in range(len(points)):
    x.append(points[i][0])
    y.append(points[i][1])

plt.scatter(x, y)
plt.show()

u_labels = np.unique(final)  # Getting unique labels
 
for i in u_labels:  # Plotting the results
    plt.scatter(points[final == i , 0] , points[final == i , 1] , final = i)
plt.legend()
plt.show()

for i in range(len(points)):
    for j in range(k):
      clusters[j] = []
      if(kmeans.labels_[i] == j):
          clusters[j].append(points(i)) # Appending points in the jth cluster



"""Q2: Hierarchical clustering"""

# 3 clusters

from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram
import numpy as np
import matplotlib.pyplot as plt

X = np.array([(1,4), (2,5), (1,2), (4,9), (0,2), (9,3), (3,4), (2,7), (3,9), (1,9), (5,8), (3,2), (2.3,4.9), (3.1,6.4),
(4.1,2.3), (2.6,8.5), (3.9, 2.3), (8,1), (2,5), (3,3), (4,4.5), (7.6,6.7), (8.4,4.8), (9.1,1.9)])
clustering3 = AgglomerativeClustering(n_clusters = 3).fit(X)
# clustering3
clustering3.labels_

import plotly.figure_factory as ff

fig = ff.create_dendrogram(X)
fig.show()

from matplotlib.pyplot import figure

figure(figsize=(12, 10), dpi=80)

def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram

    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack(
        [model.children_, model.distances_, counts]
    ).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)

# setting distance_threshold=0 ensures we compute the full tree.
model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)
# model = AgglomerativeClustering(n_clusters = 3).fit(X)

model = model.fit(X)
plt.title("Hierarchical Clustering Dendrogram")
# plotting the top three levels of the dendrogram and labeling the leaf nodes
plot_dendrogram(model, truncate_mode="level", p=3, labels = X) 
plt.xlabel("Number of points in node (or index of point if no parenthesis).")
plt.show()

# 4 clusters

clustering4 = AgglomerativeClustering(n_clusters = 4).fit(X)
clustering4
clustering4.labels_



from matplotlib.pyplot import figure

figure(figsize=(12, 10), dpi=80)

def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram

    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

   # linkage_matrix = np.column_stack(
    #    [model.children_, model.distances_, counts]
    #).astype(float)

    # Plot the corresponding dendrogram
  #  dendrogram(linkage_matrix, **kwargs)

# setting distance_threshold=0 ensures we compute the full tree.
model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)

model = model.fit(X)
plt.title("Hierarchical Clustering Dendrogram")
# plotting the top three levels of the dendrogram and labeling the leaf nodes
plot_dendrogram(model, truncate_mode="level", p=3, labels = X) 
plt.xlabel("Number of points in node (or index of point if no parenthesis).")
plt.show()



"""Image segmentation"""

pip install opencv-python

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import cv2
 
# %matplotlib inline
 

image = cv2.imread('landscape1.jpg')  # Reading in the image
 

image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Changing color to RGB (from BGR); different libraries have different conventions
 
# plt.imshow(image)
from google.colab.patches import cv2_imshow
cv2_imshow(image)

pixel_vals = image.reshape((-1,3))  # Reshaping the image into a 2D array of pixels and 3 color values (RGB)

pixel_vals = np.float32(pixel_vals)  # Converting to float type

# The below line of code defines the criteria for the algorithm to stop running, which will happen if 100 
# iterations are run or epsilon (which is the required accuracy) becomes 85%.
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.85)
 
k = 3  # Performing k-means algorithm with k set to 3
retval, labels, centers = cv2.kmeans(pixel_vals, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)  # Cluster centres are randomly initialised. 
 
centers = np.uint8(centers)  # Converting the data into 8-bit values
segmented_data = centers[labels.flatten()]
 
segmented_image = segmented_data.reshape((image.shape))  # Reshaping data into the original image dimensions
 
plt.imshow(segmented_image)

# The below line of code defines the criteria for the algorithm to stop running, which will happen if 100 
# iterations are run or epsilon (which is the required accuracy) becomes 85%.
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.85)
 
k = 4  # Performing k-means algorithm with k set to 4
retval, labels, centers = cv2.kmeans(pixel_vals, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)  # Cluster centres are randomly initialised. 
 
centers = np.uint8(centers)  # Converting the data into 8-bit values
segmented_data = centers[labels.flatten()]
 
segmented_image = segmented_data.reshape((image.shape))  # Reshaping data into the original image dimensions
 
plt.imshow(segmented_image)

# The below line of code defines the criteria for the algorithm to stop running, which will happen if 100 
# iterations are run or epsilon (which is the required accuracy) becomes 85%.
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.85)
 
k = 5  # Performing k-means algorithm with k set to 5
retval, labels, centers = cv2.kmeans(pixel_vals, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)  # Cluster centres are randomly initialised. 
 
centers = np.uint8(centers)  # Converting the data into 8-bit values
segmented_data = centers[labels.flatten()]
 
segmented_image = segmented_data.reshape((image.shape))  # Reshaping data into the original image dimensions
 
plt.imshow(segmented_image)

# The below line of code defines the criteria for the algorithm to stop running, which will happen if 100 
# iterations are run or epsilon (which is the required accuracy) becomes 85%.
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.85)
 
k = 6  # Performing k-means algorithm with k set to 6
retval, labels, centers = cv2.kmeans(pixel_vals, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)  # Cluster centres are randomly initialised. 
 
centers = np.uint8(centers)  # Converting the data into 8-bit values
segmented_data = centers[labels.flatten()]
 
segmented_image = segmented_data.reshape((image.shape))  # Reshaping data into the original image dimensions
 
plt.imshow(segmented_image)

# As can be seen clearly, the resolution of the image increases if the value of k increases.