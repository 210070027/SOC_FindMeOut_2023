# -*- coding: utf-8 -*-
"""SoC_Week3_Assg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X3AGaYpceB0v65htO1uh_sEikc8pr2I5
"""

import numpy as np
import matplotlib.pyplot as plt
import math

"""## Linear Regression"""

# define an array x of atleast 100 points with a separation of at least 1

x = np.linspace(0, 200, 100)  # Evenly spaced 100 points, between 0 and 200
x



# now choose any 2 values for A, B and generate an array y_true such that y_true = Ax + B for all x defined above

A = 0.5
B = 0.1
y_true = A*x + B # The actual functional relationship between input and output
y_true

# Add Gaussian Noise which can vary from upto -20 to 20, to y_true to get our data


noise = np.random.normal(0,1,100) # Adding standard gaussian noise
data = noise + y_true
data

# print the data, y_true and x

data

y_true

x

# plot scatterplot between data and x
# also plot y_true vs x on the same plot


plt.plot(x, y_true, color='r', label='y_true')
plt.scatter(x, data, label='data')
plt.ylabel("Corresponding Value")
plt.xlabel("x")
plt.grid()
plt.legend()
plt.show()

# write a function definition to calculate loss of linear Regression

def mse(y_true, data):
    mse = 0

    for i in range(len(data)):
        mse += (y_true[i] - data[i])**2/len(data)

    return(mse)

# write a function definition to calculate derivative of cost function of linear regression with respect to parameter A

def der_A(y_true, data, x):
    der = 0

    for i in range(len(data)):
        der += 2*(y_true[i] - data[i])*x[i]/len(data)
        
    return(der)

# write a function definition to calculate derivative of cost function of linear regression with respect to parameter B

def der_B(y_true, data):
    der = 0

    for i in range(len(data)):
        der += 2*(y_true[i] - data[i])/len(data)
        
    return(der)

# define A = 0 and B = 0 randomly
# also define 2 lists to store values of A and B at various iterations

A = 0
B = 0
val_A = [] # This array will hold selected values of A
val_B = [] # This array will hold selected values of B

# compute the model array based on initial values of A and B
# Also print the cost function for the data before training

y_true = A*x + B
data = noise + y_true
data

print("Cost: " + str(mse(y_true, data)))

# Define the Learning rate, alpha randomly. You will have to modify it during training

alpha = 0.01

# write the main training loop and train for atleast 2000 iterations
# print the cost at each iteration while training
# save the values of A and B at atleast 5 different iterations spaced by atleast 50 into the lists defined earlier
# Also, make sure you use the same model to find derivative of cost wrto A and B and then modify A and B together


value_A = []
value_B = []

y_true_new = A*x + B
tmp_A = A - alpha*der_A(y_true_new, data, x)
tmp_B = B - alpha*der_B(y_true_new, data)
print("Cost: " + str(mse(y_true_new, data)))
value_A.append(tmp_A) # This array contains all values of A, over all iterations that is
value_B.append(tmp_B) # This array contains all values of B, over all iterations that is
val_A.append(tmp_A) # This array contains selected values of A, spaced by 50 iterations
val_B.append(tmp_B) # This array contains selected values of B, spaced by 50 iterations
print(val_A)
print(val_B)


for i in range(1, 2000):
    y_true_new = value_A[i-1]*x + value_B[i-1]
    value_A.append(value_A[i-1] - alpha*der_A(y_true_new, data, x))
    value_B.append(value_B[i-1] - alpha*der_B(y_true_new, data))
    print("Cost: " + str(mse(y_true_new, data)))
    value_A.append(tmp_A) # This array contains all values of A, over all iterations that is
    value_B.append(tmp_B) # This array contains all values of B, over all iterations that is

for i in range(2000):
    if(i%50 == 0):
        val_A.append(value_A[i])
        val_B.append(value_B[i])
        print(val_A)
        print(val_B)

"""use multiple values of A and B in the initial step of generating y_true and comment on the learning rate. Also comment on how many iterations were sufficient to arrive very close to the actual solution. I hope your answer is less than 2000 ðŸ™‚"""

# print the two lists for A and B and see how those values change with number of iterations

print(val_A)
print(val_B)

len(val_A)

# generate different models for A and B you stored in the lists while training

model = []
for i in range(41):
    if(i%8 == 0): 
        model.append(val_A[i]*x + val_B[i])
        
model

# plot the different models you defined earlier and also y_true vs x on the same plot
# use different colours, though the lines may not be visible distinctly which is ok



plt.plot(x, model[0], color='r', label = 'model[0]')
plt.plot(x, model[1], color='y', label = 'model[0]')
plt.plot(x, model[2], color='b', label = 'model[0]')
plt.plot(x, model[3], color='g', label = 'model[0]')
plt.plot(x, model[4], color='c', label = 'model[0]')
plt.plot(x, model[5], color='m', label = 'model[0]')
plt.ylabel("y_true")
plt.xlabel("x")
plt.title("Models for different values of A and B")
plt.grid()
plt.legend()
plt.show()

"""So, I hope you arrive at the conclusion that if you choose the Learning rate alpha carefully you can reach very close to the actual answer in linear regression in very small number of iterations. This is important because while training a model you should always try to use the least amount of computational resources

Also, we used just one variable x in the assignment, because that makes it easier to visualize what is happening while training. But the same process works for linear regression with more than 1 variable.
"""